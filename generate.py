
# ==============================================================================
#                      AI å°æ•…äº‹ç”Ÿæˆå™¨ (ä¸»è¿è¡Œè„šæœ¬)
#
# åŠŸèƒ½: åŠ è½½ä¸€ä¸ªé¢„è®­ç»ƒçš„ TinyStoriesTransformer æ¨¡å‹å’Œ BPE åˆ†è¯å™¨ï¼Œ
#      ç„¶åæ ¹æ®ç”¨æˆ·è¾“å…¥çš„å¼€å¤´ï¼Œè‡ªåŠ¨ç”Ÿæˆå°æ•…äº‹ã€‚
# ==============================================================================

# === 1. å¯¼å…¥å¿…è¦çš„åº“ ===
import torch

# === å…³é”®: ä»ä½ åˆ›å»ºçš„æ¨¡å—ä¸­å¯¼å…¥ç±» ===
from cs336_basics.bpe import BPETokenizer
from cs336_basics.model import TinyStoriesTransformer

# === 2. ä¸»ç¨‹åºï¼šåŠ è½½æ¨¡å‹å¹¶ç”Ÿæˆæ•…äº‹ ===
def main():
    # --- ç”¨æˆ·é…ç½®åŒº (è¯·æ ¹æ®ä½ çš„æ–‡ä»¶è·¯å¾„ä¿®æ”¹) ---
    model_path = '/Users/saileisi/Downloads/ç‚¹å¤´/tinystories_checkpoints/best_model.pt'
    vocab_path = '/Users/saileisi/Downloads/ç‚¹å¤´/bpe_tokenizer_tiny/vocab.json'
    merges_path = '/Users/saileisi/Downloads/ç‚¹å¤´/bpe_tokenizer_tiny/merges.txt'
    
    print("--- AI å°æ•…äº‹ç”Ÿæˆå™¨å¯åŠ¨ ---")
    
    # --- æ­¥éª¤ 1: åŠ è½½æ¨¡å‹å’Œé…ç½® ---
    print(f"â³ æ­£åœ¨ä» '{model_path}' åŠ è½½æ¨¡å‹...")
    device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')
    checkpoint = torch.load(model_path, map_location=device)
    config = checkpoint['config']
    
    # --- æ­¥éª¤ 2: åˆå§‹åŒ–åˆ†è¯å™¨ ---
    tokenizer = BPETokenizer(vocab_path=vocab_path, merges_path=merges_path)
    eos_token_id = tokenizer.eos_token_id
    print(f"âœ… åˆ†è¯å™¨åŠ è½½å®Œæˆ (è¯æ±‡è¡¨: {tokenizer.vocab_size}, ç»“æŸID: {eos_token_id})")

    # --- æ­¥éª¤ 3: åˆå§‹åŒ–æ¨¡å‹å¹¶åŠ è½½æƒé‡ ---
    model = TinyStoriesTransformer(
        vocab_size=config['vocab_size'], d_model=config['d_model'],
        num_layers=config['num_layers'], num_heads=config['num_heads'],
        d_ff=config['d_ff'], max_seq_len=config['max_seq_len'],
        dropout=config['dropout']
    )
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(device)
    model.eval() # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
    
    num_params = sum(p.numel() for p in model.parameters())/1e6
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ (å‚æ•°é‡: {num_params:.2f}M, è®¾å¤‡: {device})")
    
    # --- æ­¥éª¤ 4: è¿›å…¥äº¤äº’å¼ç”Ÿæˆå¾ªç¯ ---
    print("\n" + "="*50)
    print("âœï¸  å¼€å§‹ç”Ÿæˆæ•…äº‹... (è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º)")
    print("="*50)
    
    while True:
        prompt = input(">>> è¯·è¾“å…¥æ•…äº‹çš„å¼€å¤´: ")
        if prompt.lower() in ['quit', 'exit']:
            print("å†è§ï¼")
            break
        
        input_ids = tokenizer.encode(prompt)
        input_tensor = torch.tensor([input_ids], dtype=torch.long, device=device)
        
        print("\n...æ¨¡å‹æ­£åœ¨åˆ›ä½œä¸­ï¼Œè¯·ç¨å€™...\n")
        
        generated_ids = model.generate(
            input_tensor,
            max_new_tokens=800,    # å¢åŠ æœ€å¤§é•¿åº¦ï¼Œç»™æ¨¡å‹è¶³å¤Ÿç©ºé—´
            temperature=0.9,      # æ§åˆ¶åˆ›é€ æ€§ï¼Œè¶Šå°è¶Šä¿å®ˆ
            top_k=100,              # åªåœ¨æ¦‚ç‡æœ€é«˜çš„100ä¸ªè¯ä¸­é€‰æ‹©
            top_p=0.9,             # æ ¸å¿ƒé‡‡æ ·ï¼Œè¿›ä¸€æ­¥ç­›é€‰
            eos_token_id=eos_token_id # å‘Šè¯‰æ¨¡å‹é‡åˆ°è¿™ä¸ªIDå°±åœæ­¢
        )
        
        generated_text = tokenizer.decode(generated_ids[0].cpu().tolist())
        
        # è¿™æ˜¯ä¸€ä¸ªå®‰å…¨æªæ–½ï¼Œä»¥é˜²æ¨¡å‹ç”Ÿæˆäº† <|endoftext|> è¿™ä¸ªç‰¹æ®Šå­—ç¬¦ä¸²
        generated_text = generated_text.split('<|endoftext|>')[0]
        
        print("--- ğŸ“– ä½ çš„å°æ•…äº‹ ğŸ“– ---")
        print(generated_text)
        print("\n" + "-"*30 + "\n")

# === 3. è„šæœ¬å…¥å£ ===
if __name__ == "__main__":
    main()
